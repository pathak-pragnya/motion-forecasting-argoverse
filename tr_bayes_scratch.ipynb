{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5s3o+lLjza+C+ywIQ2Yjn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Fh0G_I1RQSzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, npz_file):\n",
        "        data = np.load(npz_file)\n",
        "        self.past = torch.tensor(data['past'], dtype=torch.float32)\n",
        "        self.future = torch.tensor(data['future'], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.past)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.past[idx], self.future[idx]\n",
        "\n",
        "train_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/val/processed_val_pit.npz\")\n",
        "# test_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/test_obs/processed_test_pit.npz\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.shape[1], :]"
      ],
      "metadata": {
        "id": "J1dHANc0QUqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwjxo45_Nl_L"
      },
      "outputs": [],
      "source": [
        "class ProbabilisticAttentionTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=2, hidden_dim=256, num_layers=6, num_heads=8, num_samples=10, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads,\n",
        "                                                        dim_feedforward=hidden_dim * 4, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads,\n",
        "                                                        dim_feedforward=hidden_dim * 4, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.prob_attention = nn.Linear(hidden_dim, num_samples)\n",
        "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, past_trajectory):\n",
        "        batch_size, seq_len, _ = past_trajectory.shape\n",
        "        past_trajectory = self.input_proj(past_trajectory)\n",
        "        past_trajectory = self.pos_encoding(past_trajectory)\n",
        "        past_trajectory = self.layer_norm(past_trajectory)\n",
        "\n",
        "        memory = self.encoder(past_trajectory)\n",
        "        attn_weights = torch.softmax(self.prob_attention(memory), dim=-1)  # (batch, seq_len, num_samples)\n",
        "        weighted_memory = memory.unsqueeze(2) * attn_weights.unsqueeze(-1)  # (batch, seq_len, num_samples, hidden_dim)\n",
        "\n",
        "        future_preds = []\n",
        "        for i in range(self.num_samples):\n",
        "            decoded_future = self.decoder(weighted_memory[:, :, i, :], memory)\n",
        "            future_preds.append(self.output_proj(decoded_future))\n",
        "\n",
        "        return torch.stack(future_preds, dim=1), attn_weights  # (batch, num_samples, seq_len, output_dim), (batch, seq_len, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def probability_matching_loss(predicted_futures, future, attn_weights):\n",
        "    distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)  # (batch, num_samples, seq_len)\n",
        "    min_distances, indices = torch.min(distances.mean(dim=2), dim=1)  # (batch,)\n",
        "\n",
        "    true_probs = torch.zeros_like(attn_weights[:, 0, :]).scatter_(1, indices.unsqueeze(1), 1.0)  # (batch, num_samples)\n",
        "    pred_probs = attn_weights.mean(dim=1)  # Average over seq_len: (batch, num_samples)\n",
        "\n",
        "    prob_loss = torch.nn.functional.kl_div(pred_probs.log(), true_probs, reduction=\"batchmean\")\n",
        "\n",
        "    return torch.mean(min_distances), prob_loss\n",
        "\n",
        "def train_model(model, train_loader, num_epochs=50, lr=0.0005, alpha=1.0, beta=1.0, gamma=0.1, delta=0.1):\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, total_minADE, total_minFDE, total_prob_loss, total_div_loss = 0, 0, 0, 0, 0\n",
        "\n",
        "        for past, future in train_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predicted_futures, attn_weights = model(past)\n",
        "\n",
        "            # --- minADE ---\n",
        "            distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)  # (batch, num_samples, seq_len)\n",
        "            minADE_loss = torch.mean(torch.min(distances.mean(dim=2), dim=1)[0])\n",
        "\n",
        "            # --- minFDE ---\n",
        "            final_displacement = torch.norm(predicted_futures[:, :, -1, :] - future[:, -1, :].unsqueeze(1), dim=-1)\n",
        "            minFDE_loss = torch.mean(torch.min(final_displacement, dim=1)[0])\n",
        "\n",
        "            # --- Probability matching ---\n",
        "            prob_match_loss, prob_loss = probability_matching_loss(predicted_futures, future, attn_weights)\n",
        "\n",
        "            # --- Diversity ---\n",
        "            diversity_loss = torch.mean(torch.norm(predicted_futures[:, 1:, :, :] - predicted_futures[:, :-1, :, :], dim=-1))\n",
        "\n",
        "            # --- Total loss ---\n",
        "            loss = alpha * minADE_loss + beta * minFDE_loss + gamma * prob_loss + delta * diversity_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_minADE += minADE_loss.item()\n",
        "            total_minFDE += minFDE_loss.item()\n",
        "            total_prob_loss += prob_loss.item()\n",
        "            total_div_loss += diversity_loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Total Loss: {total_loss/len(train_loader):.4f} \"\n",
        "              f\"| minADE: {total_minADE/len(train_loader):.4f} | minFDE: {total_minFDE/len(train_loader):.4f} \"\n",
        "              f\"| Prob Loss: {total_prob_loss/len(train_loader):.4f} | Diversity: {total_div_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "8sWrVdwpQniu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_minADE, total_minFDE = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for past, future in test_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "            predicted_futures, attn_weights = model(past)\n",
        "\n",
        "            distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)\n",
        "            minADE = torch.mean(torch.min(distances.mean(dim=2), dim=1)[0])\n",
        "            total_minADE += minADE.item()\n",
        "\n",
        "            final_displacement = torch.norm(predicted_futures[:, :, -1, :] - future[:, -1, :].unsqueeze(1), dim=-1)\n",
        "            minFDE = torch.mean(torch.min(final_displacement, dim=1)[0])\n",
        "            total_minFDE += minFDE.item()\n",
        "\n",
        "    print(f\"Evaluation -> minADE: {total_minADE / len(test_loader):.4f}, minFDE: {total_minFDE / len(test_loader):.4f}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ProbabilisticAttentionTransformer().to(device)\n",
        "\n",
        "train_model(model, train_loader, num_epochs=50)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "GoN3T9AWQSP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}