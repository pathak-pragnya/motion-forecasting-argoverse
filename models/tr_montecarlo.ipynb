{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1n4ex8QquGNhMmhQdjIvvLR5SYlq9fQNM",
      "authorship_tag": "ABX9TyOEZOMJCsbd858FMPZSzBlh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nzNru22mQZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, npz_file):\n",
        "        data = np.load(npz_file)\n",
        "        self.past = torch.tensor(data['past'], dtype=torch.float32)\n",
        "        self.future = torch.tensor(data['future'], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.past)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.past[idx], self.future[idx]\n",
        "\n",
        "train_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/val/processed_val_pit.npz\")\n",
        "# test_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/test_obs/processed_test_pit.npz\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.shape[1], :]"
      ],
      "metadata": {
        "id": "5Qh2JORGQZx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=2, hidden_dim=256, num_layers=6, num_heads=8, num_samples=10, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads,\n",
        "                                                        dim_feedforward=hidden_dim * 4, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads,\n",
        "                                                        dim_feedforward=hidden_dim * 4, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.latent_fc = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, past_trajectory):\n",
        "        batch_size, seq_len, _ = past_trajectory.shape\n",
        "        past_trajectory = self.input_proj(past_trajectory)\n",
        "        past_trajectory = self.pos_encoding(past_trajectory)\n",
        "        past_trajectory = self.layer_norm(past_trajectory)\n",
        "\n",
        "        memory = self.encoder(past_trajectory)\n",
        "\n",
        "        future_preds = []\n",
        "        for _ in range(self.num_samples):\n",
        "            noise = torch.randn_like(memory) * 0.1\n",
        "            sampled_memory = memory + self.latent_fc(noise)\n",
        "\n",
        "            decoded_future = self.decoder(sampled_memory, sampled_memory)\n",
        "            future_preds.append(self.output_proj(decoded_future))\n",
        "\n",
        "        return torch.stack(future_preds, dim=1)  # (batch, num_samples, seq_len, output_dim)"
      ],
      "metadata": {
        "id": "InHIPcUGQZ1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIaQrvoTNgn6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, num_epochs=50, lr=0.0005, alpha=1.0, beta=1.0, gamma=0.1):\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for past, future in train_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predicted_futures = model(past)  # (batch, num_samples, seq_len, output_dim)\n",
        "\n",
        "            # --- minADE Loss ---\n",
        "            distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)  # (batch, num_samples, seq_len)\n",
        "            minADE_loss = torch.mean(torch.min(distances.mean(dim=2), dim=1)[0])  # Mean ADE\n",
        "\n",
        "            # --- minFDE Loss ---\n",
        "            final_displacement = torch.norm(predicted_futures[:, :, -1, :] - future[:, -1, :].unsqueeze(1), dim=-1)  # (batch, num_samples)\n",
        "            minFDE_loss = torch.mean(torch.min(final_displacement, dim=1)[0])  # Mean FDE\n",
        "\n",
        "            # --- Likelihood Loss ---\n",
        "            log_likelihood = -torch.logsumexp(-distances.sum(dim=2), dim=1).mean()  # Stable logsumexp\n",
        "            likelihood_loss = -log_likelihood  # Negative log likelihood\n",
        "\n",
        "            # --- Total Loss ---\n",
        "            loss = alpha * minADE_loss + beta * minFDE_loss + gamma * likelihood_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Total Loss: {epoch_loss / len(train_loader):.4f} \"\n",
        "              f\"| minADE: {minADE_loss:.4f} | minFDE: {minFDE_loss:.4f} | LL: {likelihood_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_minADE, total_minFDE = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for past, future in test_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "            predicted_futures = model(past)\n",
        "\n",
        "            # --- minADE ---\n",
        "            distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)\n",
        "            minADE = torch.mean(torch.min(distances.mean(dim=2), dim=1)[0])\n",
        "            total_minADE += minADE.item()\n",
        "\n",
        "            # --- minFDE ---\n",
        "            final_displacement = torch.norm(predicted_futures[:, :, -1, :] - future[:, -1, :].unsqueeze(1), dim=-1)\n",
        "            minFDE = torch.mean(torch.min(final_displacement, dim=1)[0])\n",
        "            total_minFDE += minFDE.item()\n",
        "\n",
        "    avg_minADE = total_minADE / len(test_loader)\n",
        "    avg_minFDE = total_minFDE / len(test_loader)\n",
        "    print(f\"Evaluation Results -> minADE: {avg_minADE:.4f}, minFDE: {avg_minFDE:.4f}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MonteCarloTransformer().to(device)\n",
        "\n",
        "train_model(model, train_loader, num_epochs=50, lr=0.0005, alpha=1.0, beta=1.0, gamma=0.1)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "EEOrO6ENPmMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
