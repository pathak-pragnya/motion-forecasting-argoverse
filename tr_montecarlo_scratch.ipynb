{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1n4ex8QquGNhMmhQdjIvvLR5SYlq9fQNM",
      "authorship_tag": "ABX9TyP7iPXhCtDbU0U0qzEis1rl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "nzNru22mQZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, npz_file):\n",
        "        data = np.load(npz_file)\n",
        "        self.past = torch.tensor(data['past'], dtype=torch.float32)\n",
        "        self.future = torch.tensor(data['future'], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.past)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.past[idx], self.future[idx]\n",
        "\n",
        "train_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/val/processed_val_pit.npz\")\n",
        "# test_dataset = TrajectoryDataset(\"/content/drive/MyDrive/ECE271B project/test_obs/processed_test_pit.npz\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "5Qh2JORGQZx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2], pe[:, 1::2] = torch.sin(position * div_term), torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        B, N, _ = x.shape\n",
        "        context = x if context is None else context\n",
        "        qkv = self.qkv(torch.cat([x, context], dim=1)).chunk(3, dim=-1)\n",
        "        q, k, v = [t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
        "        scores = (q @ k.transpose(-2, -1)) / self.scale\n",
        "        attn = self.dropout(scores.softmax(-1))\n",
        "        return self.fc_out((attn @ v).transpose(1, 2).reshape(B, N, -1))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = CustomAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        x = self.norm1(x + self.mha(x, context))\n",
        "        return self.norm2(x + self.ff(x))\n",
        "\n",
        "\n",
        "class DualPathStochasticDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, num_samples):\n",
        "        super().__init__()\n",
        "        self.coarse_layers = nn.ModuleList([TransformerLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.refine_layers = nn.ModuleList([TransformerLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_token = nn.Parameter(torch.randn(1, 1, d_model))  # Per-token learnable stochastic embedding\n",
        "\n",
        "    def forward(self, queries, memory):\n",
        "        batch, seq, _ = queries.size()\n",
        "        outputs = []\n",
        "\n",
        "        for _ in range(self.num_samples):\n",
        "            noise = torch.randn(batch, seq, memory.size(-1), device=queries.device) * 0.1\n",
        "            conditioned_noise = memory + noise\n",
        "            x = queries + self.latent_token\n",
        "\n",
        "            # Coarse decoding\n",
        "            for layer in self.coarse_layers:\n",
        "                x = layer(x, conditioned_noise)\n",
        "\n",
        "            # Fine refinement\n",
        "            for layer in self.refine_layers:\n",
        "                x = layer(x, conditioned_noise + x)\n",
        "\n",
        "            outputs.append(x)\n",
        "\n",
        "        return torch.stack(outputs, dim=1)  # (batch, num_samples, seq, d_model)\n",
        "\n",
        "\n",
        "class MonteCarloTransformerScratch(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model=256, num_layers=4, num_heads=8, d_ff=1024, num_samples=10):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.output_proj = nn.Linear(d_model, output_dim)\n",
        "        self.encoder = nn.Sequential(*[TransformerLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.decoder = DualPathStochasticDecoder(num_layers, d_model, num_heads, d_ff, num_samples)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "    def forward(self, past_trajectory, future_queries):\n",
        "        x = self.pos_encoding(self.input_proj(past_trajectory))\n",
        "        memory = self.encoder(x)\n",
        "        decoded = self.decoder(future_queries, memory)\n",
        "        return self.output_proj(decoded)\n",
        "\n",
        "\n",
        "def losses(predicted_futures, future_gt, alpha=1.0, beta=1.0, gamma=0.1, delta=0.1):\n",
        "    distances = torch.norm(predicted_futures - future_gt.unsqueeze(1), dim=-1)\n",
        "    minADE = torch.mean(torch.min(distances.mean(2), 1)[0])\n",
        "    minFDE = torch.mean(torch.min(distances[:, :, -1], 1)[0])\n",
        "    log_likelihood = -torch.logsumexp(-distances.sum(2), 1).mean()\n",
        "\n",
        "    # Contrastive loss for diversity\n",
        "    diffs = predicted_futures.unsqueeze(2) - predicted_futures.unsqueeze(1)\n",
        "    diversity = torch.mean(torch.norm(diffs, dim=-1))  # Average pairwise distance\n",
        "\n",
        "    total_loss = alpha * minADE + beta * minFDE - gamma * log_likelihood - delta * diversity\n",
        "    return total_loss, minADE, minFDE, -log_likelihood, diversity"
      ],
      "metadata": {
        "id": "InHIPcUGQZ1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, num_epochs=50, lr=0.0005, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for past, future in train_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            future_queries = torch.zeros_like(future).to(device)\n",
        "            predicted_futures = model(past, future_queries)\n",
        "            loss, minADE, minFDE, nll, diversity = losses(predicted_futures, future)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {total_loss/len(train_loader):.4f} | minADE: {minADE:.4f} | \"\n",
        "              f\"minFDE: {minFDE:.4f} | LL: {nll:.4f} | Diversity: {diversity:.4f}\")"
      ],
      "metadata": {
        "id": "XPTNNJkiz-OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_minADE, total_minFDE = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for past, future in test_loader:\n",
        "            past, future = past.to(device), future.to(device)\n",
        "            predicted_futures = model(past)\n",
        "\n",
        "            # --- minADE ---\n",
        "            distances = torch.norm(predicted_futures - future.unsqueeze(1), dim=-1)\n",
        "            minADE = torch.mean(torch.min(distances.mean(dim=2), dim=1)[0])\n",
        "            total_minADE += minADE.item()\n",
        "\n",
        "            # --- minFDE ---\n",
        "            final_displacement = torch.norm(predicted_futures[:, :, -1, :] - future[:, -1, :].unsqueeze(1), dim=-1)\n",
        "            minFDE = torch.mean(torch.min(final_displacement, dim=1)[0])\n",
        "            total_minFDE += minFDE.item()\n",
        "\n",
        "    avg_minADE = total_minADE / len(test_loader)\n",
        "    avg_minFDE = total_minFDE / len(test_loader)\n",
        "    print(f\"Evaluation Results -> minADE: {avg_minADE:.4f}, minFDE: {avg_minFDE:.4f}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MonteCarloTransformerScratch().to(device)\n",
        "\n",
        "train_model(model, train_loader, num_epochs=50, lr=0.0005, alpha=1.0, beta=1.0, gamma=0.1)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "EEOrO6ENPmMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}